# Ch02-模型评估与选择 - Agent视角

## 📊 评估指标的数学基础

### 经验误差与泛化误差

#### 经验误差 (Empirical Error)
$$\hat{R}(h) = \frac{1}{n} \sum_{i=1}^n L(y_i, h(x_i))$$

其中：
- $h$ 是假设函数
- $L$ 是损失函数
- $n$ 是训练样本数量

#### 泛化误差 (Generalization Error)
$$R(h) = \mathbb{E}_{(x,y) \sim D}[L(y, h(x))]$$

#### 误差分解
$$R(h) = \text{Bias}^2(h) + \text{Var}(h) + \text{Noise}$$

- **偏差**: 模型的期望预测与真实值的差异
- **方差**: 模型预测的波动性
- **噪声**: 数据本身的噪声

### 偏差-方差权衡

#### 偏差-方差困境
- **高偏差**: 模型过于简单，无法捕捉数据中的模式
- **高方差**: 模型过于复杂，对训练数据过拟合
- **权衡**: 需要在偏差和方差之间找到平衡点

#### 数学表达
$$\mathbb{E}[(y - \hat{f}(x))^2] = \text{Bias}^2[\hat{f}(x)] + \text{Var}[\hat{f}(x)] + \sigma^2$$

其中：
- $\text{Bias}[\hat{f}(x)] = \mathbb{E}[\hat{f}(x)] - f(x)$
- $\text{Var}[\hat{f}(x)] = \mathbb{E}[\hat{f}(x)^2] - \mathbb{E}[\hat{f}(x)]^2$
- $\sigma^2$ 是噪声方差

## 🎯 分类问题评估指标

### 二分类评估

#### 混淆矩阵
```
                预测正例    预测负例
实际正例        TP           FN
实际负例        FP           TN
```

#### 基本指标
- **准确率**: $\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}$
- **精确率**: $\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}$
- **召回率**: $\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}$
- **F1分数**: $\text{F1} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$

#### 综合指标
- **马修斯相关系数**: 
$$\text{MCC} = \frac{\text{TP} \times \text{TN} - \text{FP} \times \text{FN}}{\sqrt{(\text{TP}+\text{FP})(\text{TP}+\text{FN})(\text{TN}+\text{FP})(\text{TN}+\text{FN})}}$$

- **Kappa系数**: 衡量分类器与随机分类的一致性

### 多分类评估

#### 宏平均 (Macro-averaging)
- **宏精确率**: $\text{Macro-P} = \frac{1}{K} \sum_{k=1}^K P_k$
- **宏召回率**: $\text{Macro-R} = \frac{1}{K} \sum_{k=1}^K R_k$
- **宏F1**: $\text{Macro-F1} = \frac{1}{K} \sum_{k=1}^K F1_k$

#### 微平均 (Micro-averaging)
- **微精确率**: $\text{Micro-P} = \frac{\sum_{k=1}^K \text{TP}_k}{\sum_{k=1}^K (\text{TP}_k + \text{FP}_k)}$
- **微召回率**: $\text{Micro-R} = \frac{\sum_{k=1}^K \text{TP}_k}{\sum_{k=1}^K (\text{TP}_k + \text{FN}_k)}$
- **微F1**: $\text{Micro-F1} = 2 \times \frac{\text{Micro-P} \times \text{Micro-R}}{\text{Micro-P} + \text{Micro-R}}$

## 📈 回归问题评估指标

### 基础指标

#### 均方误差 (MSE)
$$\text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2$$

#### 平均绝对误差 (MAE)
$$\text{MAE} = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i|$$

#### 均方根误差 (RMSE)
$$\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}$$

### 相对指标

#### 决定系数 (R²)
$$R^2 = 1 - \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^n (y_i - \bar{y})^2}$$

#### 调整R²
$$\bar{R}^2 = 1 - (1-R^2)\frac{n-1}{n-p-1}$$

其中 $p$ 是特征数量。

#### 平均绝对百分比误差 (MAPE)
$$\text{MAPE} = \frac{100\%}{n} \sum_{i=1}^n \left|\frac{y_i - \hat{y}_i}{y_i}\right|$$

## 🔄 交叉验证技术

### k折交叉验证

#### 算法步骤
1. 将数据集随机分成k个大小相等的子集
2. 对于每个子集i：
   - 使用第i个子集作为验证集
   - 使用其余k-1个子集作为训练集
   - 训练模型并计算验证误差
3. 计算k次验证误差的平均值

#### 数学表达
$$\text{CV}_k = \frac{1}{k} \sum_{i=1}^k \frac{1}{|D_i|} \sum_{(x,y) \in D_i} L(y, h^{(-i)}(x))$$

其中 $h^{(-i)}$ 是在不包含第i个子集的数据上训练的模型。

#### 复杂度分析
- **时间复杂度**: $O(k \times T_{\text{train}})$
- **空间复杂度**: $O(|D|)$
- **计算代价**: 随着k的增加而增加

### 留一法交叉验证 (LOOCV)

#### 特点
- $k = n$，每个样本单独作为验证集
- 几乎无偏，但计算成本高
- 适合小数据集

#### 数学表达
$$\text{LOOCV} = \frac{1}{n} \sum_{i=1}^n L(y_i, h^{(-i)}(x_i))$$

### 自助法 (Bootstrap)

#### 重采样技术
1. 从训练集中有放回地采样n个样本
2. 使用采样的样本训练模型
3. 使用未采样的样本进行验证
4. 重复多次，计算平均性能

#### 数学表达
$$\text{Bootstrap} = \frac{1}{B} \sum_{b=1}^B \frac{1}{|D_b^c|} \sum_{(x,y) \in D_b^c} L(y, h_b(x))$$

其中 $D_b$ 是第b次采样的训练集，$D_b^c$ 是对应的验证集。

## 🎛️ 假设检验

### 统计显著性检验

#### t检验
用于比较两个模型的性能差异是否显著。

$$t = \frac{\bar{x}_1 - \bar{x}_2}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}$$

其中：
- $\bar{x}_1, \bar{x}_2$ 是两个模型的平均性能
- $s_1^2, s_2^2$ 是两个性能的方差
- $n_1, n_2$ 是测试次数

#### 方差分析 (ANOVA)
用于比较多个模型的性能差异。

$$F = \frac{\text{MS}_{\text{between}}}{\text{MS}_{\text{within}}}$$

### 置信区间

#### 性能差异的置信区间
$$\text{CI} = (\bar{d} - t_{\alpha/2} \times \frac{s_d}{\sqrt{n}}, \bar{d} + t_{\alpha/2} \times \frac{s_d}{\sqrt{n}})$$

其中：
- $\bar{d}$ 是性能差异的平均值
- $s_d$ 是性能差异的标准差
- $n$ 是测试次数

## 🤖 Agent系统中的实际应用

### 在线评估策略

#### 流式数据评估
```python
class OnlineEvaluator:
    def __init__(self, window_size=1000):
        self.window_size = window_size
        self.recent_predictions = []
        self.recent_labels = []
    
    def add_result(self, prediction, label):
        self.recent_predictions.append(prediction)
        self.recent_labels.append(label)
        
        if len(self.recent_predictions) > self.window_size:
            self.recent_predictions.pop(0)
            self.recent_labels.pop(0)
    
    def get_current_metrics(self):
        if len(self.recent_predictions) < 100:
            return None
        
        return self.calculate_metrics(self.recent_predictions, self.recent_labels)
```

#### 延迟反馈处理
```python
class DelayedFeedbackEvaluator:
    def __init__(self, max_delay=3600):
        self.max_delay = max_delay
        self.pending_predictions = {}
    
    def add_prediction(self, prediction_id, prediction, timestamp):
        self.pending_predictions[prediction_id] = {
            'prediction': prediction,
            'timestamp': timestamp
        }
    
    def add_feedback(self, prediction_id, label, feedback_timestamp):
        if prediction_id in self.pending_predictions:
            pred_info = self.pending_predictions[prediction_id]
            delay = feedback_timestamp - pred_info['timestamp']
            
            if delay <= self.max_delay:
                self.evaluate_with_delay(
                    pred_info['prediction'], 
                    label, 
                    delay
                )
            
            del self.pending_predictions[prediction_id]
```

### 多目标优化

#### 帕累托最优
```python
class MultiObjectiveOptimizer:
    def __init__(self, objectives):
        self.objectives = objectives
    
    def is_pareto_dominated(self, point, other_points):
        for other in other_points:
            if all(other[obj] >= point[obj] for obj in self.objectives) and \
               any(other[obj] > point[obj] for obj in self.objectives):
                return True
        return False
    
    def find_pareto_front(self, points):
        pareto_front = []
        for point in points:
            if not self.is_pareto_dominated(point, points):
                pareto_front.append(point)
        return pareto_front
```

#### 加权和方法
```python
def weighted_sum_score(metrics, weights):
    """
    多目标加权评分
    metrics: dict of metric_name -> value
    weights: dict of metric_name -> weight
    """
    total_score = 0
    for metric_name, value in metrics.items():
        if metric_name in weights:
            total_score += weights[metric_name] * value
    return total_score
```

### 模型选择策略

#### 基于复杂度的选择
```python
def select_model_by_complexity(candidates, complexity_threshold):
    """
    基于复杂度阈值选择模型
    """
    acceptable_models = []
    for model in candidates:
        if model.complexity <= complexity_threshold:
            acceptable_models.append(model)
    
    if not acceptable_models:
        # 如果没有符合复杂度要求的模型，选择最简单的
        return min(candidates, key=lambda x: x.complexity)
    
    # 在可接受的模型中选择性能最好的
    return max(acceptable_models, key=lambda x: x.performance)
```

#### 基于资源约束的选择
```python
def select_model_with_constraints(candidates, constraints):
    """
    基于资源约束选择模型
    constraints: dict of resource_name -> max_value
    """
    feasible_models = []
    
    for model in candidates:
        feasible = True
        for resource, max_value in constraints.items():
            if getattr(model, resource) > max_value:
                feasible = False
                break
        
        if feasible:
            feasible_models.append(model)
    
    if not feasible_models:
        return None
    
    return max(feasible_models, key=lambda x: x.performance)
```

## 🔧 性能优化技术

### 计算效率优化

#### 增量评估
```python
class IncrementalEvaluator:
    def __init__(self):
        self.total_metrics = {}
        self.count = 0
    
    def add_batch(self, predictions, labels):
        batch_metrics = self.calculate_metrics(predictions, labels)
        
        if self.count == 0:
            self.total_metrics = batch_metrics
        else:
            for metric in batch_metrics:
                self.total_metrics[metric] = (
                    self.total_metrics[metric] * self.count + 
                    batch_metrics[metric] * len(predictions)
                ) / (self.count + len(predictions))
        
        self.count += len(predictions)
        return self.total_metrics
```

#### 并行评估
```python
from concurrent.futures import ThreadPoolExecutor
import numpy as np

class ParallelEvaluator:
    def __init__(self, n_workers=4):
        self.n_workers = n_workers
    
    def parallel_evaluate(self, model, test_data_chunks):
        def evaluate_chunk(chunk):
            predictions = model.predict(chunk['X'])
            return self.calculate_metrics(predictions, chunk['y'])
        
        with ThreadPoolExecutor(max_workers=self.n_workers) as executor:
            results = list(executor.map(evaluate_chunk, test_data_chunks))
        
        return self.aggregate_metrics(results)
```

### 内存效率优化

#### 流式评估
```python
class StreamingEvaluator:
    def __init__(self, chunk_size=1000):
        self.chunk_size = chunk_size
        self.metrics_aggregator = None
    
    def evaluate_streaming(self, model, data_stream):
        chunk = []
        
        for sample in data_stream:
            chunk.append(sample)
            
            if len(chunk) >= self.chunk_size:
                chunk_metrics = self.evaluate_chunk(model, chunk)
                self.metrics_aggregator = self.aggregate_chunk_metrics(
                    self.metrics_aggregator, chunk_metrics
                )
                chunk = []
        
        # 处理剩余数据
        if chunk:
            chunk_metrics = self.evaluate_chunk(model, chunk)
            self.metrics_aggregator = self.aggregate_chunk_metrics(
                self.metrics_aggregator, chunk_metrics
            )
        
        return self.metrics_aggregator
```

## 📊 实现考虑

### 代码结构设计

#### 评估器基类
```python
from abc import ABC, abstractmethod
import numpy as np

class BaseEvaluator(ABC):
    def __init__(self):
        self.metrics_history = []
    
    @abstractmethod
    def calculate_metrics(self, predictions, labels):
        """计算评估指标"""
        pass
    
    def evaluate_model(self, model, test_data):
        """评估模型性能"""
        predictions = model.predict(test_data['X'])
        metrics = self.calculate_metrics(predictions, test_data['y'])
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self):
        """获取平均性能"""
        if not self.metrics_history:
            return None
        
        avg_metrics = {}
        for metric in self.metrics_history[0]:
            avg_metrics[metric] = np.mean([
                m[metric] for m in self.metrics_history
            ])
        
        return avg_metrics
```

#### 分类评估器
```python
class ClassificationEvaluator(BaseEvaluator):
    def calculate_metrics(self, predictions, labels):
        from sklearn.metrics import (
            accuracy_score, precision_score, 
            recall_score, f1_score, roc_auc_score
        )
        
        metrics = {
            'accuracy': accuracy_score(labels, predictions),
            'precision': precision_score(labels, predictions, average='weighted'),
            'recall': recall_score(labels, predictions, average='weighted'),
            'f1': f1_score(labels, predictions, average='weighted')
        }
        
        # 如果是二分类，添加AUC
        if len(np.unique(labels)) == 2:
            try:
                metrics['auc'] = roc_auc_score(labels, predictions)
            except:
                metrics['auc'] = 0.5
        
        return metrics
```

#### 回归评估器
```python
class RegressionEvaluator(BaseEvaluator):
    def calculate_metrics(self, predictions, labels):
        from sklearn.metrics import (
            mean_squared_error, mean_absolute_error, r2_score
        )
        
        metrics = {
            'mse': mean_squared_error(labels, predictions),
            'mae': mean_absolute_error(labels, predictions),
            'rmse': np.sqrt(mean_squared_error(labels, predictions)),
            'r2': r2_score(labels, predictions)
        }
        
        return metrics
```

## 📝 学习总结

### 关键洞察
1. **理论基础**: 扎实的数学理论是正确选择评估指标的基础
2. **实践考量**: 在实际应用中需要考虑计算复杂度和资源限制
3. **上下文相关**: 不同的应用场景需要不同的评估策略

### 实践计划
- [ ] 实现一个完整的模型评估框架
- [ ] 优化评估过程的计算效率
- [ ] 在Agent系统中应用在线评估技术

## 🤔 技术疑问

### 理论问题
- 如何在非独立同分布数据上进行有效的模型评估？
- 如何处理类别不平衡问题对评估指标的影响？
- 如何设计适合Agent系统的增量评估方法？

### 实现问题
- 如何在高维空间中有效计算评估指标？
- 如何处理实时数据流中的模型漂移检测？
- 如何设计高效的交叉验证并行算法？

## 📊 相关链接

**共享讨论区**: [[💬专题讨论区/Ch02-模型评估与选择-讨论]]

**学习资源**: [[🔗学习资料汇总]]

**个人笔记**: 
- [[Ch01-绪论|上一章]]
- [[Ch03-线性模型|下一章]]
- [[📚核心术语表|术语表]]