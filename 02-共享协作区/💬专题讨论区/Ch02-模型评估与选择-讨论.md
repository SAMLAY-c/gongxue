# 💬 Ch02-模型评估与选择-讨论

## 疑问与探讨 (Q&A)

### 待解决问题
- [ ] 如何在实际项目中选择合适的评估指标？
- [ ] 交叉验证的具体实现和调参策略
- [ ] 处理类别不平衡问题的最佳实践
- [ ] 模型选择时如何平衡偏差和方差？

### 已解决的问题
- [x] 过拟合和欠拟合的概念和区别
- [x] 各种评估指标的计算方法和适用场景

## 视角碰撞 (Perspective Clash)

### 🎯 AI PM 视角

**业务价值思考**：
- **产品场景**: 在推荐系统中，精确率和召回率的权衡如何影响用户满意度？
- **商业指标**: 
  - 高精确率 → 减少错误推荐，提升用户信任
  - 高召回率 → 增加曝光机会，提升商业收益
- **用户体验**: 不同业务场景下，用户对误报和漏报的容忍度如何？

**产品功能设计**：
- **A/B测试框架**: 如何设计实验来验证不同评估指标的优化效果？
- **监控dashboard**: 哪些评估指标需要实时监控，哪些只需要周期性查看？
- **用户反馈**: 如何将用户反馈与模型评估指标关联起来？

**商业决策支持**：
- **成本收益分析**: 误报和漏报的成本如何量化？
- **阈值调优**: 如何根据业务需求动态调整分类阈值？
- **模型迭代**: 何时需要重新训练和评估模型？

### 🧑‍🔬 AI Agent 视角

**技术实现思考**：
- **算法选择**: 不同评估指标对应的优化算法是什么？
- **计算复杂度**: 交叉验证的计算成本如何优化？
- **并行计算**: 如何利用并行计算加速模型评估过程？

**工程实践问题**：
- **特征工程**: 哪些特征对特定评估指标影响最大？
- **数据预处理**: 不同的预处理方法如何影响评估结果？
- **模型调参**: 自动化调参策略（如网格搜索、贝叶斯优化）的优缺点？

**系统架构设计**：
- **在线评估**: 如何设计在线A/B测试系统？
- **模型监控**: 实时性能监控和异常检测的实现方案
- **版本管理**: 模型版本和评估结果的版本控制策略

## 🔍 深入讨论

### 评估指标的选择策略

**PM视角**:
- 推荐系统可能更关注精确率（避免推荐用户不感兴趣的内容）
- 搜索引擎可能更关注召回率（避免遗漏相关结果）
- 风控系统需要同时关注精确率和召回率

**Agent视角**:
- 不同的评估指标对应不同的损失函数
- 多目标优化问题的解决方案
- 评估指标的数学特性和优化难度

### 实际项目中的权衡

**业务约束**:
- 计算资源限制
- 延迟要求
- 准确性要求

**技术约束**:
- 数据质量
- 特征工程难度
- 模型复杂度

## 📊 相关链接

**个人笔记链接**:
- [[👩‍💻[你的名字]-AI PM/Ch02-模型评估与选择]]
- [[🧑‍🔬[朋友的名字]-Agent/Ch02-模型评估与选择]]

**参考资料**:
- [Scikit-learn评估指标文档](https://scikit-learn.org/stable/modules/model_evaluation.html)
- [机器学习评估指标详解](https://zhuanlan.zhihu.com/p/280445582)

## 📝 行动项

### 待讨论问题
- [ ] 下次会议讨论具体的评估指标选择策略
- [ ] 分享各自项目中模型评估的实践经验

### 学习任务
- [ ] 阅读相关论文，了解评估指标的最新研究
- [ ] 实践不同评估指标的实现和比较
- [ ] 准备一个实际案例进行分析